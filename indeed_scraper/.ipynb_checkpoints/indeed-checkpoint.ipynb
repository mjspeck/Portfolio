{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary classifier.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title, and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use a random forest classifier, as well as another classifier of your choice; either logistic regression, SVM, or KNN. \n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com. In the second, we'll focus on using listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\")\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "The URL here has many query parameters\n",
    "- q for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- l for a location\n",
    "- start for what result number to start on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to extract location from Indeed search result\n",
    "def location_res(result):\n",
    "    tag = result.find(name='span', attrs={'class':'location'}) #find appropriate tag\n",
    "    try:\n",
    "        if re.search(\"<\",tag.renderContents()): #helps clean the data and only extract actual text instead of html code\n",
    "            return tag.find(name='span', attrs={'itemprop':'addressLocality'}).renderContents()\n",
    "        else:\n",
    "            return entry.renderContents()\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to extract company name from Indeed search result\n",
    "def company_res(result):\n",
    "    tag = result.find(name='span', attrs={'itemprop':'name'}) #find appropriate tag\n",
    "    try: #First try statement accounts for whether there is any company at all\n",
    "        try: #Second try statement accounts for whether there any nested tags\n",
    "            return tag.find('a').renderContents()\n",
    "        except:\n",
    "            return tag.renderContents()\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract job title\n",
    "def job_res(result):\n",
    "    try: #Accounts for missing job title\n",
    "        tag = result.find(name='a', attrs={'data-tn-element':'jobTitle'})\n",
    "        return tag.renderContents()\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract salary\n",
    "def salary_res(result):\n",
    "    tag = result.find(name='td', attrs={'class':'snip'})\n",
    "    tag2 = tag.find('nobr')\n",
    "    try: # Try statement is especially important for this function since most results don't have a salary\n",
    "        return tag2.renderContents()\n",
    "    except:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function iterates through each result on a single Indeed.com results page then applies the four functions\n",
    "# above to extract the relevant information. It takes a search argument in order to also keep track of the search\n",
    "# term used, since location can give a different value than the actual city or location searched.\n",
    "def all_funcs(search):\n",
    "    entries=[]\n",
    "    for result in soup.find_all(name='div', attrs={'class':' row result'}):\n",
    "        result_data=[]\n",
    "        result_data.append(job_res(result))\n",
    "        result_data.append(company_res(result))\n",
    "        result_data.append(location_res(result))\n",
    "        result_data.append(salary_res(result))\n",
    "        result_data.append(search)\n",
    "        entries.append(result_data)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-11 23:43:35.312408\n",
      "Bethesda%2C+MD DONE\n",
      "2017-04-11 23:50:22.142442 0:06:46.830191\n",
      "Morrisville%2C+NC DONE\n",
      "2017-04-11 23:56:46.531761 0:13:11.219413\n",
      "Palo+Alto%2C+CA DONE\n",
      "2017-04-12 00:03:40.606056 0:20:05.293708\n",
      "Redmond%2C+WA DONE\n",
      "2017-04-12 00:10:35.961321 0:27:00.649018\n",
      "Mountain+View%2C+CA DONE\n",
      "2017-04-12 00:17:45.693266 0:34:10.380940\n",
      "El+Segundo%2C+CA DONE\n",
      "2017-04-12 00:24:42.294780 0:41:06.983958\n",
      "Herndon%2C+VA DONE\n",
      "2017-04-12 00:31:36.120337 0:48:00.807994\n",
      "Menlo+Park%2C+CA DONE\n",
      "2017-04-12 00:38:50.463017 0:55:15.150910\n",
      "Collegeville%2C+PA DONE\n",
      "2017-04-12 00:45:59.129439 1:02:23.817304\n",
      "Roseland%2C+NJ DONE\n",
      "2017-04-12 00:53:01.092451 1:09:25.780103\n",
      "Princeton%2C+NJ DONE\n",
      "2017-04-12 00:59:42.040190 1:16:06.727843\n",
      "St.+Louis%2C+MO DONE\n",
      "2017-04-12 01:06:21.505255 1:22:46.192909\n",
      "Tampa%2C+FL DONE\n",
      "2017-04-12 01:13:21.407556 1:29:46.095421\n",
      "Cambridge%2C+MA DONE\n",
      "2017-04-12 01:20:17.836725 1:36:42.524396\n",
      "Stamford%2C+CT DONE\n",
      "2017-04-12 01:27:10.784225 1:43:35.471877\n",
      "Santa+Clara%2C+CA DONE\n",
      "2017-04-12 01:34:06.775713 1:50:31.463384\n",
      "Detroit DONE\n",
      "2017-04-12 01:40:55.923566 1:57:20.611215\n",
      "Ann+Arbor%2C+MI DONE\n",
      "2017-04-12 01:47:47.739718 2:04:12.427377\n",
      "Des+Moines%2C+IA DONE\n",
      "2017-04-12 01:54:39.002705 2:11:03.690568\n",
      "Minneapolis%2C+MN DONE\n",
      "2017-04-12 02:01:35.465563 2:18:00.153237\n",
      "New+Orleans DONE\n",
      "2017-04-12 02:08:12.447044 2:24:37.134917\n",
      "2:24:37.135241\n"
     ]
    }
   ],
   "source": [
    "max_results_per_city = 3000\n",
    "results = [] #Empty list that will contain all results\n",
    "a = datetime.datetime.now() # Start time of process\n",
    "print a\n",
    "city_list = ['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington%2C+DC', \n",
    "    'Baltimore', 'El+Paso', 'Boston','Bethesda%2C+MD','Morrisville%2C+NC',\n",
    "    'Palo+Alto%2C+CA','Redmond%2C+WA','Mountain+View%2C+CA','El+Segundo%2C+CA',\n",
    "    'Herndon%2C+VA','Menlo+Park%2C+CA', 'Collegeville%2C+PA','Roseland%2C+NJ',\n",
    "    'Princeton%2C+NJ','St.+Louis%2C+MO', 'Tampa%2C+FL','Cambridge%2C+MA',\n",
    "    'Stamford%2C+CT','Santa+Clara%2C+CA','Detroit', 'Ann+Arbor%2C+MI', \n",
    "    'Des+Moines%2C+IA', 'Minneapolis%2C+MN','New+Orleans']\n",
    "\n",
    "for city in city_list: # Iterate through cities\n",
    "    for start in range(0, max_results_per_city, 10): #Iterate through results pages\n",
    "        url=\"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=\" + city + \"&start=\" + str(start)\n",
    "        html = urllib.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        data = all_funcs(city) #use functions from before to extract all job listing info\n",
    "        for i in range(len(data)): #add info to results list\n",
    "            results.append(data[i])\n",
    "        sleep(1)\n",
    "    print city + \" DONE\"\n",
    "    print \"Elapsed time: \" + str(datetime.datetime.now() - a) #Update user on progress\n",
    "        \n",
    "b = datetime.datetime.now()\n",
    "c = b - a\n",
    "print c\n",
    "\n",
    "#Turn results list into dataframe\n",
    "df = pd.DataFrame(results,columns=['Job Title','Company','Location','Salary','Search Term'])\n",
    "\n",
    "df.to_csv('FILEPATH/FILENAME.csv') #Save data\n",
    "\n",
    "del results #Remove results list from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting salaries using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "Here I'm loading in several csvs of data I scraped over the course of three days. I ran this program several times, expanding the list of cities and changing the maximum number of results. I never took any cities off the list, even though this meant that numerous duplicates showed up. My thinking behind this was that I'd like to catch as many job listings as possible, so if some more went up in certain cities, I could catch those, and remove the duplicates later on, which I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/overnight_monday.csv')\n",
    "# df2 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/tues_morn_20.csv')\n",
    "# df3 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/tues_morn_class.csv')\n",
    "# df4 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/second_major_run.csv')\n",
    "# df5 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/tues_overnight_run.csv')\n",
    "# df6 = pd.read_csv('/Users/mjspeck/Desktop/Indeed_data/first_run.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.concat([df1, df2, df3, df4, df5, df6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.drop('Unnamed: 0',axis=1,inplace=True) #Every time a df is saved to a csv, it creates an unnamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.drop_duplicates(inplace=True) #This dropped LOTS of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the purpose of this project was to calculate the median salary of a data scientist and then use NLP to predict whether a given job listing would offer a salary above or below the median, any listing without a salary had to be dropped. It was a shame since at least 90% of the job listings did not contain salary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create df containing only observations that had a salary value\n",
    "df_salary = pd.DataFrame(master_df[master_df.Salary.isnull() == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_salary.Company.replace(to_replace='\\n', value='',inplace=True, regex=True) #Remove useless characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_salary['Job Title'].replace(to_replace=['<b>','</b>'], value='', inplace=True, regex=True) #Remove useless characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More characters to remove\n",
    "df_salary['Search Term'].replace(to_replace='\\+', value = ' ', inplace=True, regex=True)\n",
    "df_salary['Search Term'].replace(to_replace='\\%2C', value = ',', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing all characters between and including < >\n",
    "df_salary['Location'].replace(to_replace='<.*?>',value = '', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>$80,000 - $120,000 a year</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>$50,000 - $80,000 a year</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Engage Partners</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>$120 an hour</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>$43,794 a year</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>$46,831 a year</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Job Title                             Company  \\\n",
       "0                         Data Scientist                  Platinum Solutions   \n",
       "17     Bioinformatics Software Developer                            Genialis   \n",
       "19                        Data Scientist                     Engage Partners   \n",
       "22  Quality Assurance Analyst (Research)          Baylor College of Medicine   \n",
       "32           Flow Cytometry Specialist I          Baylor College of Medicine   \n",
       "\n",
       "       Location                     Salary Search Term  \n",
       "0   Houston, TX  $80,000 - $120,000 a year     Houston  \n",
       "17  Houston, TX   $50,000 - $80,000 a year     Houston  \n",
       "19  Houston, TX               $120 an hour     Houston  \n",
       "22  Houston, TX             $43,794 a year     Houston  \n",
       "32  Houston, TX             $46,831 a year     Houston  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how the clean data looks\n",
    "df_salary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, even though I had already taken out all observations without a salary value, I had to cut down the data even further by removing entries with hourly wages instead of a salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove all rows with hourly salary\n",
    "df_salary_yearly = df_salary[df_salary.Salary.str.contains('hour',case=False) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take dollar signs out \n",
    "df_salary_yearly['Salary'].replace(to_replace=['\\$',',',' a year'], value='', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "0                                      Data Scientist   \n",
       "17                  Bioinformatics Software Developer   \n",
       "22               Quality Assurance Analyst (Research)   \n",
       "32                        Flow Cytometry Specialist I   \n",
       "52  Research Statistical Analyst -Bioinformatics &...   \n",
       "\n",
       "                               Company     Location          Salary  \\\n",
       "0                   Platinum Solutions  Houston, TX  80000 - 120000   \n",
       "17                            Genialis  Houston, TX   50000 - 80000   \n",
       "22          Baylor College of Medicine  Houston, TX           43794   \n",
       "32          Baylor College of Medicine  Houston, TX           46831   \n",
       "52           MD Anderson Cancer Center  Houston, TX   66400 - 99600   \n",
       "\n",
       "   Search Term  \n",
       "0      Houston  \n",
       "17     Houston  \n",
       "22     Houston  \n",
       "32     Houston  \n",
       "52     Houston  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I had a cleaned up salary column, I had to deal with the salary ranges. To do this, I wrote two functions that could be applied to the salary column. One would take the lower value, the other would take the upper value. Then, I used the returned values from these functions to make two columns for the upper and lower values of the salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_func_lower(val):\n",
    "    return val.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_func_upper(val):\n",
    "    val.split('-')\n",
    "    try:\n",
    "        return val.split('-')[1]\n",
    "    except:\n",
    "        return val.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower_col=df_salary_yearly.Salary.apply(split_func_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_col = df_salary_yearly.Salary.apply(split_func_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "0                                      Data Scientist   \n",
       "17                  Bioinformatics Software Developer   \n",
       "22               Quality Assurance Analyst (Research)   \n",
       "32                        Flow Cytometry Specialist I   \n",
       "52  Research Statistical Analyst -Bioinformatics &...   \n",
       "\n",
       "                               Company     Location          Salary  \\\n",
       "0                   Platinum Solutions  Houston, TX  80000 - 120000   \n",
       "17                            Genialis  Houston, TX   50000 - 80000   \n",
       "22          Baylor College of Medicine  Houston, TX           43794   \n",
       "32          Baylor College of Medicine  Houston, TX           46831   \n",
       "52           MD Anderson Cancer Center  Houston, TX   66400 - 99600   \n",
       "\n",
       "   Search Term lower_sal_val upper_sal_val  \n",
       "0      Houston        80000         120000  \n",
       "17     Houston        50000          80000  \n",
       "22     Houston         43794         43794  \n",
       "32     Houston         46831         46831  \n",
       "52     Houston        66400          99600  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Everything looks good\n",
    "df_salary_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing spaces so that column dtypes will be ints\n",
    "df_salary_yearly['lower_sal_val'].replace(to_replace=' ', value='', inplace=True, regex=True)\n",
    "df_salary_yearly['upper_sal_val'].replace(to_replace=' ', value='', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job Title        object\n",
       "Company          object\n",
       "Location         object\n",
       "Salary           object\n",
       "Search Term      object\n",
       "lower_sal_val    object\n",
       "upper_sal_val    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I realized that I had not accounted for monthly or weekly salaries, so I filtered those observations out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Also did this for 'month' and 'week'\n",
    "#df_salary_yearly = df_salary_yearly[df_salary_yearly.Salary.str.contains('day',case=False) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_salary_yearly[['lower_sal_val','upper_sal_val']] = df_salary_yearly[['lower_sal_val','upper_sal_val']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "      <th>mean salary</th>\n",
       "      <th>median_bool</th>\n",
       "      <th>60_perc_bool</th>\n",
       "      <th>70_perc_bool</th>\n",
       "      <th>75_perc_bool</th>\n",
       "      <th>90_perc_bool</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Career Evolutions</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>150000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>RESEARCH AND STATISTICAL ANALYST</td>\n",
       "      <td>Arizona Health Care Cost Containment S...</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>39983 - 55500</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>39983</td>\n",
       "      <td>55500</td>\n",
       "      <td>47741.5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>AIR QUALITY PLANNER 2-3</td>\n",
       "      <td>State of Arizona</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>45000 - 77000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>45000</td>\n",
       "      <td>77000</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Paralegal</td>\n",
       "      <td>Emord &amp;amp; Associates, P.C.</td>\n",
       "      <td>Gilbert, AZ 85295</td>\n",
       "      <td>45000 - 55000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>45000</td>\n",
       "      <td>55000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>ENVIRONMENTAL SCIENTIST - EMISSIONS</td>\n",
       "      <td>State of Arizona</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>49000 - 61000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>49000</td>\n",
       "      <td>61000</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0                                       Data Scientist   \n",
       "17                   Bioinformatics Software Developer   \n",
       "22                Quality Assurance Analyst (Research)   \n",
       "32                         Flow Cytometry Specialist I   \n",
       "52   Research Statistical Analyst -Bioinformatics &...   \n",
       "148                                     Data Architect   \n",
       "168                   RESEARCH AND STATISTICAL ANALYST   \n",
       "181                            AIR QUALITY PLANNER 2-3   \n",
       "191                                          Paralegal   \n",
       "198                ENVIRONMENTAL SCIENTIST - EMISSIONS   \n",
       "\n",
       "                                               Company           Location  \\\n",
       "0                                   Platinum Solutions        Houston, TX   \n",
       "17                                            Genialis        Houston, TX   \n",
       "22                          Baylor College of Medicine        Houston, TX   \n",
       "32                          Baylor College of Medicine        Houston, TX   \n",
       "52                           MD Anderson Cancer Center        Houston, TX   \n",
       "148                                  Career Evolutions        Phoenix, AZ   \n",
       "168          Arizona Health Care Cost Containment S...        Phoenix, AZ   \n",
       "181                                   State of Arizona        Phoenix, AZ   \n",
       "191                       Emord &amp; Associates, P.C.  Gilbert, AZ 85295   \n",
       "198                                   State of Arizona        Phoenix, AZ   \n",
       "\n",
       "             Salary Search Term  lower_sal_val  upper_sal_val  mean salary  \\\n",
       "0    80000 - 120000     Houston          80000         120000     100000.0   \n",
       "17    50000 - 80000     Houston          50000          80000      65000.0   \n",
       "22            43794     Houston          43794          43794      43794.0   \n",
       "32            46831     Houston          46831          46831      46831.0   \n",
       "52    66400 - 99600     Houston          66400          99600      83000.0   \n",
       "148          150000     Phoenix         150000         150000     150000.0   \n",
       "168   39983 - 55500     Phoenix          39983          55500      47741.5   \n",
       "181   45000 - 77000     Phoenix          45000          77000      61000.0   \n",
       "191   45000 - 55000     Phoenix          45000          55000      50000.0   \n",
       "198   49000 - 61000     Phoenix          49000          61000      55000.0   \n",
       "\n",
       "    median_bool 60_perc_bool 70_perc_bool 75_perc_bool 90_perc_bool state  \n",
       "0         False        False        False        False        False  'TX'  \n",
       "17        False        False        False        False        False  'TX'  \n",
       "22        False        False        False        False        False  'TX'  \n",
       "32        False        False        False        False        False  'TX'  \n",
       "52        False        False        False        False        False  'TX'  \n",
       "148        True         True         True         True        False  'AZ'  \n",
       "168       False        False        False        False        False  'AZ'  \n",
       "181       False        False        False        False        False  'AZ'  \n",
       "191       False        False        False        False        False  'AZ'  \n",
       "198       False        False        False        False        False  'AZ'  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_salary_yearly['mean salary'] = 0.5*df_salary_yearly.lower_sal_val + 0.5*df_salary_yearly.upper_sal_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108607.5"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(df_salary_yearly['mean salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to return median salary boolean\n",
    "def above_median(val):\n",
    "    if val > np.median(df_salary_yearly['mean salary']):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_salary_yearly['median_bool'] = df_salary_yearly['mean salary'].apply(above_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_salary_yearly.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't _have_ to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_list=[60,70,75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140000.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(df_salary_yearly['mean salary'],q=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# Numpy percentile: https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n",
    "def above_percentile(val):\n",
    "    if val > np.percentile(df_salary_yearly['mean salary'],q=90):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_salary_yearly['90_perc_bool'] = df_salary_yearly['mean salary'].apply(above_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "      <th>mean salary</th>\n",
       "      <th>median_bool</th>\n",
       "      <th>60_perc_bool</th>\n",
       "      <th>70_perc_bool</th>\n",
       "      <th>75_perc_bool</th>\n",
       "      <th>90_perc_bool</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[TX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[TX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[TX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[TX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[TX]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "0                                      Data Scientist   \n",
       "17                  Bioinformatics Software Developer   \n",
       "22               Quality Assurance Analyst (Research)   \n",
       "32                        Flow Cytometry Specialist I   \n",
       "52  Research Statistical Analyst -Bioinformatics &...   \n",
       "\n",
       "                               Company     Location          Salary  \\\n",
       "0                   Platinum Solutions  Houston, TX  80000 - 120000   \n",
       "17                            Genialis  Houston, TX   50000 - 80000   \n",
       "22          Baylor College of Medicine  Houston, TX           43794   \n",
       "32          Baylor College of Medicine  Houston, TX           46831   \n",
       "52           MD Anderson Cancer Center  Houston, TX   66400 - 99600   \n",
       "\n",
       "   Search Term  lower_sal_val  upper_sal_val  mean salary median_bool  \\\n",
       "0      Houston          80000         120000     100000.0       False   \n",
       "17     Houston          50000          80000      65000.0       False   \n",
       "22     Houston          43794          43794      43794.0       False   \n",
       "32     Houston          46831          46831      46831.0       False   \n",
       "52     Houston          66400          99600      83000.0       False   \n",
       "\n",
       "   60_perc_bool 70_perc_bool 75_perc_bool 90_perc_bool state  \n",
       "0         False        False        False        False  [TX]  \n",
       "17        False        False        False        False  [TX]  \n",
       "22        False        False        False        False  [TX]  \n",
       "32        False        False        False        False  [TX]  \n",
       "52        False        False        False        False  [TX]  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CA'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to take out state\n",
    "# http://stackoverflow.com/questions/7090717/regular-expression-to-match-3-capital-letters-followed-by-a-small-letter-followe\n",
    "re.findall('[A-Z]{2}',df_salary_yearly.Location.iloc[387])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract states\n",
    "df_salary_yearly['state'] = df_salary_yearly.Location.str.findall('[A-Z]{2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Somehow this was a list type\n",
    "df_salary_yearly['state'] = df_salary_yearly['state'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove brackets from states\n",
    "df_salary_yearly['state'].replace(to_replace=['\\[','\\]'], value='',inplace=True,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_salary_yearly.to_csv('../csvs/df_state2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "      <th>mean salary</th>\n",
       "      <th>median_bool</th>\n",
       "      <th>60_perc_bool</th>\n",
       "      <th>70_perc_bool</th>\n",
       "      <th>75_perc_bool</th>\n",
       "      <th>90_perc_bool</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Career Evolutions</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>150000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>RESEARCH AND STATISTICAL ANALYST</td>\n",
       "      <td>Arizona Health Care Cost Containment S...</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>39983 - 55500</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>39983</td>\n",
       "      <td>55500</td>\n",
       "      <td>47741.5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>AIR QUALITY PLANNER 2-3</td>\n",
       "      <td>State of Arizona</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>45000 - 77000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>45000</td>\n",
       "      <td>77000</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Paralegal</td>\n",
       "      <td>Emord &amp;amp; Associates, P.C.</td>\n",
       "      <td>Gilbert, AZ 85295</td>\n",
       "      <td>45000 - 55000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>45000</td>\n",
       "      <td>55000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>ENVIRONMENTAL SCIENTIST - EMISSIONS</td>\n",
       "      <td>State of Arizona</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>49000 - 61000</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>49000</td>\n",
       "      <td>61000</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'AZ'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job Title  \\\n",
       "0                                       Data Scientist   \n",
       "17                   Bioinformatics Software Developer   \n",
       "22                Quality Assurance Analyst (Research)   \n",
       "32                         Flow Cytometry Specialist I   \n",
       "52   Research Statistical Analyst -Bioinformatics &...   \n",
       "148                                     Data Architect   \n",
       "168                   RESEARCH AND STATISTICAL ANALYST   \n",
       "181                            AIR QUALITY PLANNER 2-3   \n",
       "191                                          Paralegal   \n",
       "198                ENVIRONMENTAL SCIENTIST - EMISSIONS   \n",
       "\n",
       "                                               Company           Location  \\\n",
       "0                                   Platinum Solutions        Houston, TX   \n",
       "17                                            Genialis        Houston, TX   \n",
       "22                          Baylor College of Medicine        Houston, TX   \n",
       "32                          Baylor College of Medicine        Houston, TX   \n",
       "52                           MD Anderson Cancer Center        Houston, TX   \n",
       "148                                  Career Evolutions        Phoenix, AZ   \n",
       "168          Arizona Health Care Cost Containment S...        Phoenix, AZ   \n",
       "181                                   State of Arizona        Phoenix, AZ   \n",
       "191                       Emord &amp; Associates, P.C.  Gilbert, AZ 85295   \n",
       "198                                   State of Arizona        Phoenix, AZ   \n",
       "\n",
       "             Salary Search Term  lower_sal_val  upper_sal_val  mean salary  \\\n",
       "0    80000 - 120000     Houston          80000         120000     100000.0   \n",
       "17    50000 - 80000     Houston          50000          80000      65000.0   \n",
       "22            43794     Houston          43794          43794      43794.0   \n",
       "32            46831     Houston          46831          46831      46831.0   \n",
       "52    66400 - 99600     Houston          66400          99600      83000.0   \n",
       "148          150000     Phoenix         150000         150000     150000.0   \n",
       "168   39983 - 55500     Phoenix          39983          55500      47741.5   \n",
       "181   45000 - 77000     Phoenix          45000          77000      61000.0   \n",
       "191   45000 - 55000     Phoenix          45000          55000      50000.0   \n",
       "198   49000 - 61000     Phoenix          49000          61000      55000.0   \n",
       "\n",
       "    median_bool 60_perc_bool 70_perc_bool 75_perc_bool 90_perc_bool state  \n",
       "0         False        False        False        False        False  'TX'  \n",
       "17        False        False        False        False        False  'TX'  \n",
       "22        False        False        False        False        False  'TX'  \n",
       "32        False        False        False        False        False  'TX'  \n",
       "52        False        False        False        False        False  'TX'  \n",
       "148        True         True         True         True        False  'AZ'  \n",
       "168       False        False        False        False        False  'AZ'  \n",
       "181       False        False        False        False        False  'AZ'  \n",
       "191       False        False        False        False        False  'AZ'  \n",
       "198       False        False        False        False        False  'AZ'  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "Hmmmmmmmmmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a Random Forest model to predict High/Low salary using Sklearn. Start by ONLY using the location as a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Resetting indeces since they are the indeces from the original 10,000+ observation dataset\n",
    "df_salary_yearly.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "#Do two models: predict using Search Term, and using State\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df_salary_yearly['median_bool'])\n",
    "X = pd.get_dummies(df_salary_yearly['Search Term']) #Not sure if I need to drop_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_salary_yearly['Search Term'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly['Search Term'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = pd.get_dummies(df_salary_yearly['state'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 23)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=20, max_depth=20)\n",
    "#clf_fit = clf.fit(X,y)\n",
    "scores = cross_val_score(clf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68181818  0.45454545  0.39393939  0.24242424  0.35384615  0.671875\n",
      "  0.3125      0.3125      0.40625     0.359375  ]\n",
      "0.140262901274\n",
      "0.418907342657\n"
     ]
    }
   ],
   "source": [
    "#Super shitty using only location\n",
    "print scores\n",
    "print np.std(scores)\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.lmplot(x='X', y='Y', hue='PdDistrict', fit_reg=False, data=sf_crime)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for class:\n",
      "{0: 0.35938136638046259}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "logreg_cv = LogisticRegressionCV(Cs=10, cv=10, penalty='l1', scoring='accuracy', solver='liblinear')\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "print('best C for class:')\n",
    "best_C = {logreg_cv.classes_[i]:x for i, (x, c) in enumerate(zip(logreg_cv.C_, logreg_cv.classes_))}\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_2 = cross_val_score(logreg_cv, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56521739  0.58695652  0.65217391  0.60869565  0.55555556  0.57777778\n",
      "  0.48888889  0.64444444  0.55555556  0.6       ]\n",
      "0.0449770907428\n",
      "0.583526570048\n"
     ]
    }
   ],
   "source": [
    "print scores_2\n",
    "print np.std(scores_2)\n",
    "print np.mean(scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need a bunch of models! Good Random Forest and Logistic Regression, use gridsearch, classification report (4.06), NLP (count vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a job title.\n",
    "- For example, create a feature that represents whether 'Senior' is in the title \n",
    "- or whether 'Manager' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "068dc1cf-7fd7-4f27-a1f1-7f0a5a221d29"
   },
   "outputs": [],
   "source": [
    "#Import CV method\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fitting the vectorizer on our training data\n",
    "cvec.fit(df_salary_yearly['Job Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_data = pd.DataFrame(cvec.transform(df_salary_yearly['Job Title']).todense(),\n",
    "                       columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>04</th>\n",
       "      <th>10</th>\n",
       "      <th>10b</th>\n",
       "      <th>17096</th>\n",
       "      <th>1809</th>\n",
       "      <th>196</th>\n",
       "      <th>2017</th>\n",
       "      <th>500</th>\n",
       "      <th>...</th>\n",
       "      <th>visual</th>\n",
       "      <th>visualization</th>\n",
       "      <th>vp</th>\n",
       "      <th>waste</th>\n",
       "      <th>water</th>\n",
       "      <th>web</th>\n",
       "      <th>wellness</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  01  04  10  10b  17096  1809  196  2017  500  ...    visual  \\\n",
       "0    0   0   0   0    0      0     0    0     0    0  ...         0   \n",
       "1    0   0   0   0    0      0     0    0     0    0  ...         0   \n",
       "2    0   0   0   0    0      0     0    0     0    0  ...         0   \n",
       "3    0   0   0   0    0      0     0    0     0    0  ...         0   \n",
       "4    0   0   0   0    0      0     0    0     0    0  ...         0   \n",
       "\n",
       "   visualization  vp  waste  water  web  wellness  working  world  youth  \n",
       "0              0   0      0      0    0         0        0      0      0  \n",
       "1              0   0      0      0    0         0        0      0      0  \n",
       "2              0   0      0      0    0         0        0      0      0  \n",
       "3              0   0      0      0    0         0        0      0      0  \n",
       "4              0   0      0      0    0         0        0      0      0  \n",
       "\n",
       "[5 rows x 459 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data            257\n",
       "scientist       241\n",
       "analyst         154\n",
       "research        122\n",
       "engineer         95\n",
       "senior           92\n",
       "learning         53\n",
       "machine          51\n",
       "software         34\n",
       "manager          33\n",
       "amp              32\n",
       "quantitative     25\n",
       "science          24\n",
       "developer        24\n",
       "statistical      23\n",
       "sr               23\n",
       "principal        20\n",
       "analytics        19\n",
       "lead             18\n",
       "clinical         18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = cvec_data.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 459)\n",
      "(649, 24)\n"
     ]
    }
   ],
   "source": [
    "print cvec_data.shape\n",
    "print X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'AZ'</th>\n",
       "      <th>'CA'</th>\n",
       "      <th>'CO'</th>\n",
       "      <th>'CT'</th>\n",
       "      <th>'DC'</th>\n",
       "      <th>'DE'</th>\n",
       "      <th>'FL'</th>\n",
       "      <th>'GA'</th>\n",
       "      <th>'IA'</th>\n",
       "      <th>'IL'</th>\n",
       "      <th>...</th>\n",
       "      <th>'MN'</th>\n",
       "      <th>'MO'</th>\n",
       "      <th>'NC'</th>\n",
       "      <th>'NJ'</th>\n",
       "      <th>'NY'</th>\n",
       "      <th>'OR'</th>\n",
       "      <th>'PA'</th>\n",
       "      <th>'TX'</th>\n",
       "      <th>'VA'</th>\n",
       "      <th>'WA'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   'AZ'  'CA'  'CO'  'CT'  'DC'  'DE'  'FL'  'GA'  'IA'  'IL'  ...   'MN'  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   'MO'  'NC'  'NJ'  'NY'  'OR'  'PA'  'TX'  'VA'  'WA'  \n",
       "0     0     0     0     0     0     0     1     0     0  \n",
       "1     0     0     0     0     0     0     1     0     0  \n",
       "2     0     0     0     0     0     0     1     0     0  \n",
       "3     0     0     0     0     0     0     1     0     0  \n",
       "4     0     0     0     0     0     0     1     0     0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an X matrix that has state dummy variables as well as CountVec vals\n",
    "X_state = pd.concat([cvec_data, X2], axis=1, join_axes=[X_state.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 482)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_search = pd.concat([cvec_data, X], axis=1, join_axes=[X.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 496)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do a test train split using cvec data as well as search term dummy variables\n",
    "X_search_train, X_search_test, y_search_train, y_search_test = train_test_split(X_search, y, test_size=0.3)\n",
    "logreg_cv = LogisticRegressionCV(Cs=10, cv=10, penalty='l1', scoring='accuracy', solver='liblinear')\n",
    "logreg_cv.fit(X_search_train, y_search_train)\n",
    "\n",
    "#take accuracy scores of fit\n",
    "scores_search = cross_val_score(logreg_cv, X_search_train, y_search_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82978723  0.73913043  0.86956522  0.71111111  0.84444444  0.86666667\n",
      "  0.8         0.88888889  0.86666667  0.84444444]\n",
      "0.0559258104138\n",
      "0.826070510844\n"
     ]
    }
   ],
   "source": [
    "print scores_search\n",
    "print np.std(scores_search)\n",
    "print np.mean(scores_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84782609  0.84782609  0.82608696  0.86956522  0.89130435  0.91304348\n",
      "  0.88888889  0.82222222  0.77272727  0.68181818]\n",
      "0.0642349086314\n",
      "0.836130873957\n"
     ]
    }
   ],
   "source": [
    "#Do the same thing, but use state instead of search term\n",
    "X_state_train, X_state_test, y_state_train, y_state_test = train_test_split(X_state, y, test_size=0.3)\n",
    "logreg_cv = LogisticRegressionCV(Cs=10, cv=10, penalty='l1', scoring='accuracy', solver='liblinear')\n",
    "logreg_cv.fit(X_state_train, y_state_train)\n",
    "\n",
    "scores_state = cross_val_score(logreg_cv, X_state_train, y_state_train, cv=10)\n",
    "\n",
    "print scores_state\n",
    "print np.std(scores_state)\n",
    "print np.mean(scores_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87878788  0.77272727  0.81818182  0.86363636  0.76923077  0.921875\n",
      "  0.828125    0.90625     0.796875    0.703125  ]\n",
      "0.064736306181\n",
      "0.825881410256\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=20, max_depth=20)\n",
    "#clf_fit = clf.fit(X_search_train,y_search_train)\n",
    "scores = cross_val_score(clf, X_search, y, cv=10)\n",
    "print scores\n",
    "print np.std(scores)\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.28512387e-03,   1.03014902e-04,   0.00000000e+00,\n",
       "         5.26625567e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   2.49281028e-04,   6.26085828e-04,\n",
       "         1.66224463e-04,   2.93816839e-05,   6.81213879e-05,\n",
       "         6.11649443e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.62467301e-03,   1.53420496e-03,\n",
       "         1.14476170e-03,   1.02753429e-03,   6.84865078e-04,\n",
       "         6.00417953e-04,   0.00000000e+00,   4.53716969e-04,\n",
       "         2.78168455e-03,   4.90522685e-03,   6.23840084e-02,\n",
       "         0.00000000e+00,   1.18093084e-05,   2.02894965e-03,\n",
       "         7.87072140e-04,   9.43095447e-05,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.00549200e-03,   1.98934196e-04,\n",
       "         1.97259742e-03,   1.98554822e-03,   1.95587743e-03,\n",
       "         0.00000000e+00,   3.58700154e-03,   0.00000000e+00,\n",
       "         2.16826940e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         3.50195600e-06,   2.40131689e-03,   0.00000000e+00,\n",
       "         1.59857335e-03,   5.69004563e-04,   4.88113543e-05,\n",
       "         0.00000000e+00,   2.46953950e-05,   6.02528224e-03,\n",
       "         2.75563554e-03,   6.68806725e-04,   0.00000000e+00,\n",
       "         2.09702536e-03,   1.33527940e-05,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         9.82558461e-04,   4.17558151e-05,   4.19750825e-04,\n",
       "         0.00000000e+00,   3.25241668e-03,   5.31220182e-03,\n",
       "         0.00000000e+00,   1.26990129e-03,   5.84184915e-05,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   7.41821888e-06,   1.31031630e-04,\n",
       "         0.00000000e+00,   4.88729715e-06,   2.07712605e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   8.06260179e-03,\n",
       "         6.36589427e-04,   7.75714159e-05,   1.34753663e-03,\n",
       "         4.10962841e-05,   1.20766188e-05,   2.00859327e-03,\n",
       "         0.00000000e+00,   7.73358348e-04,   3.71132655e-04,\n",
       "         4.12078855e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   8.14953354e-03,   2.00476908e-03,\n",
       "         0.00000000e+00,   1.18755784e-05,   2.76630898e-06,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.31009837e-01,\n",
       "         7.30153446e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         3.96679716e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         5.69244224e-03,   2.71135623e-03,   0.00000000e+00,\n",
       "         8.97535337e-04,   0.00000000e+00,   2.11840466e-03,\n",
       "         8.26303447e-03,   0.00000000e+00,   1.51620819e-05,\n",
       "         2.86948785e-05,   2.08866721e-03,   1.61154633e-03,\n",
       "         0.00000000e+00,   1.47731991e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.15986798e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         2.40986853e-04,   0.00000000e+00,   7.99829921e-04,\n",
       "         5.28982475e-05,   4.03922043e-02,   9.39070231e-04,\n",
       "         0.00000000e+00,   0.00000000e+00,   3.77409192e-03,\n",
       "         6.75215983e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         2.58201843e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   2.28566693e-03,   9.15841505e-04,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         8.04057732e-05,   9.55087908e-04,   1.63807535e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   6.37177372e-04,\n",
       "         6.94099902e-04,   6.07071705e-05,   0.00000000e+00,\n",
       "         0.00000000e+00,   2.77192645e-03,   0.00000000e+00,\n",
       "         6.76899801e-04,   4.71418323e-04,   0.00000000e+00,\n",
       "         1.27254964e-04,   5.07618024e-04,   0.00000000e+00,\n",
       "         1.07338989e-03,   0.00000000e+00,   4.37211721e-03,\n",
       "         1.24977490e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         6.91120943e-04,   0.00000000e+00,   1.36900453e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   3.52913021e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.31818108e-05,\n",
       "         4.32410029e-04,   0.00000000e+00,   3.84443011e-04,\n",
       "         1.36878833e-04,   4.79913219e-03,   4.02077907e-03,\n",
       "         0.00000000e+00,   3.51264852e-03,   2.64788020e-03,\n",
       "         0.00000000e+00,   7.23501301e-04,   2.46803202e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.30059419e-03,\n",
       "         4.12651769e-05,   6.51249061e-04,   0.00000000e+00,\n",
       "         6.99963706e-04,   0.00000000e+00,   7.07321585e-05,\n",
       "         0.00000000e+00,   1.18968539e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   3.39941144e-05,   2.13753298e-03,\n",
       "         7.68223714e-04,   0.00000000e+00,   1.43234348e-03,\n",
       "         4.79694485e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   2.02362913e-03,   8.42160950e-04,\n",
       "         2.47747711e-03,   4.61086799e-04,   2.91078743e-03,\n",
       "         7.13154473e-04,   6.92277241e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         4.13570345e-04,   4.08362146e-05,   5.50534954e-04,\n",
       "         1.49819977e-02,   6.84579569e-04,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   6.20341152e-03,\n",
       "         1.07185076e-03,   4.10975628e-02,   0.00000000e+00,\n",
       "         8.88125014e-05,   4.64934825e-03,   0.00000000e+00,\n",
       "         0.00000000e+00,   3.04999386e-04,   3.70917603e-03,\n",
       "         1.74123480e-03,   0.00000000e+00,   1.30679841e-04,\n",
       "         0.00000000e+00,   2.61032524e-02,   0.00000000e+00,\n",
       "         5.63771142e-04,   0.00000000e+00,   4.67997882e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   4.88552338e-04,\n",
       "         6.86659545e-04,   2.20695725e-03,   6.31517313e-04,\n",
       "         1.14161848e-03,   5.87542293e-04,   1.25589242e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   2.29391090e-03,\n",
       "         2.62312608e-05,   3.47511713e-04,   5.54569170e-04,\n",
       "         1.50007306e-06,   1.60288188e-03,   5.43327315e-04,\n",
       "         0.00000000e+00,   1.17175969e-03,   1.61546189e-03,\n",
       "         0.00000000e+00,   1.36289226e-04,   5.91204860e-04,\n",
       "         6.52857594e-04,   5.32556083e-05,   4.72392765e-05,\n",
       "         3.35631962e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         6.96999154e-05,   0.00000000e+00,   0.00000000e+00,\n",
       "         8.92638729e-04,   1.16747750e-03,   4.58856063e-05,\n",
       "         1.16836983e-04,   0.00000000e+00,   3.40840137e-05,\n",
       "         3.47119783e-05,   0.00000000e+00,   2.79172689e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.00600595e-03,\n",
       "         5.62840056e-05,   0.00000000e+00,   7.25885298e-04,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         8.33715232e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.43814618e-04,   0.00000000e+00,   2.70173881e-03,\n",
       "         8.17890657e-04,   0.00000000e+00,   4.95392947e-04,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.98263949e-03,\n",
       "         1.32272003e-03,   2.47434456e-03,   0.00000000e+00,\n",
       "         1.83943974e-03,   5.40002538e-05,   0.00000000e+00,\n",
       "         1.24423869e-02,   4.05533324e-04,   1.59152284e-03,\n",
       "         1.22866734e-03,   0.00000000e+00,   6.64719946e-03,\n",
       "         1.37303796e-03,   5.84772738e-03,   1.18609641e-03,\n",
       "         2.08370630e-07,   0.00000000e+00,   0.00000000e+00,\n",
       "         9.24783625e-04,   0.00000000e+00,   3.85349486e-03,\n",
       "         2.12511235e-03,   5.81559090e-04,   2.96150242e-03,\n",
       "         6.91712101e-03,   1.24483904e-04,   1.33944082e-03,\n",
       "         0.00000000e+00,   3.57745426e-06,   2.36499880e-03,\n",
       "         3.70844521e-03,   4.18187016e-04,   1.44424693e-03,\n",
       "         4.60207944e-02,   1.90138459e-03,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   2.98434574e-05,\n",
       "         8.32767514e-03,   0.00000000e+00,   0.00000000e+00,\n",
       "         3.23923503e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         5.48193572e-03,   5.16136499e-04,   1.09825745e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.91397616e-02,\n",
       "         3.34182987e-04,   0.00000000e+00,   2.99850495e-02,\n",
       "         0.00000000e+00,   3.17811922e-03,   2.48854885e-04,\n",
       "         2.87579308e-02,   9.91531912e-04,   3.74278541e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         8.87940893e-04,   6.37292634e-06,   7.66804342e-04,\n",
       "         7.32961436e-03,   0.00000000e+00,   7.02883929e-04,\n",
       "         0.00000000e+00,   3.25043990e-03,   0.00000000e+00,\n",
       "         1.20218960e-03,   5.02728389e-03,   4.02632828e-03,\n",
       "         8.09633688e-04,   4.63735935e-03,   3.18403719e-04,\n",
       "         4.31001904e-03,   2.06847055e-05,   5.74115774e-05,\n",
       "         1.74191528e-06,   5.21283702e-03,   6.53350777e-04,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   7.61042297e-06,   5.71196892e-04,\n",
       "         1.49237965e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.94985159e-03,   2.87736060e-03,\n",
       "         0.00000000e+00,   2.59479954e-03,   2.72119174e-04,\n",
       "         1.44724789e-03,   5.04879650e-03,   0.00000000e+00,\n",
       "         0.00000000e+00,   3.41359588e-03,   0.00000000e+00,\n",
       "         5.16377677e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         2.62299017e-04,   1.42299091e-03,   9.27278388e-04,\n",
       "         7.01186061e-04,   0.00000000e+00,   1.60047783e-03,\n",
       "         3.80546068e-04,   0.00000000e+00,   1.69768949e-03,\n",
       "         0.00000000e+00,   5.58026575e-03,   0.00000000e+00,\n",
       "         2.62396799e-04,   7.11987096e-04,   8.54790771e-04,\n",
       "         3.21799395e-05,   1.52208459e-05,   1.53607858e-04,\n",
       "         5.23560414e-04,   1.46536929e-03,   1.87805240e-03,\n",
       "         1.59521252e-03,   5.68613188e-03,   3.96998811e-03,\n",
       "         3.17413727e-03,   8.09341917e-04,   6.28985752e-03,\n",
       "         8.58247307e-03,   1.68778984e-03,   2.69734243e-03,\n",
       "         2.32606855e-05,   0.00000000e+00,   2.82604824e-03,\n",
       "         2.15864370e-03,   2.66351790e-03,   4.68028067e-03,\n",
       "         2.89034232e-03,   8.43339710e-03,   5.34913336e-03,\n",
       "         1.13106434e-03,   4.05501111e-03,   3.80944174e-03,\n",
       "         9.63684618e-03,   5.85336318e-03,   1.37335545e-03,\n",
       "         2.30895625e-03,   7.55008239e-04,   1.89175765e-04,\n",
       "         9.47519303e-03,   6.73336527e-04,   2.24624995e-03,\n",
       "         9.23055044e-03,   1.40224125e-02,   8.59602913e-04,\n",
       "         6.48645004e-03,   2.08747929e-03,   3.36681492e-05,\n",
       "         4.02736266e-03])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cvec_train, X_cvec_test, y_cvec_train, y_cvec_test = train_test_split(cvec_data, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=20, max_depth=20)\n",
    "#clf_fit = clf.fit(X_cvec_train, y_cvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.89130435  0.7826087   0.86956522  0.86956522  0.82608696  0.95652174\n",
      "  0.86666667  0.88888889  0.79545455  0.79545455]\n",
      "0.0514733922469\n",
      "0.854211682038\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X_cvec_train, y_cvec_train,cv=10)\n",
    "print scores\n",
    "print np.std(scores)\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=20, max_depth=20)\n",
    "clf_fit = clf.fit(X_cvec_train, y_cvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(clf_fit, cvec_data, y, cv=10)\n",
    "accuracy = metrics.accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y, predictions)\n",
    "sensitivity = metrics.recall_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875192604006\n",
      "Precision: 0.885714285714\n",
      "Sensitivity: 0.861111111111\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print \"Accuracy: \" + str(accuracy)\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Sensitivity: \" + str(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.089219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.068727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.065603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>0.063372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.059086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           importance\n",
       "data         0.089219\n",
       "scientist    0.068727\n",
       "learning     0.065603\n",
       "analyst      0.063372\n",
       "research     0.059086"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = cvec_data.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features = feature_importances.head(50).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features_df = pd.DataFrame(feature_importances.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp2 = []\n",
    "for i in important_features:\n",
    "    imp2.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print len(imp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES! 6\n"
     ]
    }
   ],
   "source": [
    "n=1\n",
    "for i in imp2:\n",
    "    if re.search('engineer',i):\n",
    "        print \"YES!\", n\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "      <th>mean salary</th>\n",
       "      <th>median_bool</th>\n",
       "      <th>60_perc_bool</th>\n",
       "      <th>70_perc_bool</th>\n",
       "      <th>75_perc_bool</th>\n",
       "      <th>90_perc_bool</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                                     Data Scientist   \n",
       "1                  Bioinformatics Software Developer   \n",
       "2               Quality Assurance Analyst (Research)   \n",
       "3                        Flow Cytometry Specialist I   \n",
       "4  Research Statistical Analyst -Bioinformatics &...   \n",
       "\n",
       "                              Company     Location          Salary  \\\n",
       "0                  Platinum Solutions  Houston, TX  80000 - 120000   \n",
       "1                            Genialis  Houston, TX   50000 - 80000   \n",
       "2          Baylor College of Medicine  Houston, TX           43794   \n",
       "3          Baylor College of Medicine  Houston, TX           46831   \n",
       "4           MD Anderson Cancer Center  Houston, TX   66400 - 99600   \n",
       "\n",
       "  Search Term  lower_sal_val  upper_sal_val  mean salary median_bool  \\\n",
       "0     Houston          80000         120000     100000.0       False   \n",
       "1     Houston          50000          80000      65000.0       False   \n",
       "2     Houston          43794          43794      43794.0       False   \n",
       "3     Houston          46831          46831      46831.0       False   \n",
       "4     Houston          66400          99600      83000.0       False   \n",
       "\n",
       "  60_perc_bool 70_perc_bool 75_perc_bool 90_perc_bool state  \n",
       "0        False        False        False        False  'TX'  \n",
       "1        False        False        False        False  'TX'  \n",
       "2        False        False        False        False  'TX'  \n",
       "3        False        False        False        False  'TX'  \n",
       "4        False        False        False        False  'TX'  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_salary_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create column with boolean values, representing whether word appears in job title\n",
    "for i in imp2:\n",
    "    df_salary_yearly[i + ' bool'] = df_salary_yearly['Job Title'].str.contains(i,case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130000.0"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(df_salary_yearly[df_salary_yearly['data bool'] == True]['mean salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_medians = []\n",
    "word_means = []\n",
    "for i in imp2:\n",
    "    word_medians.append(np.median(df_salary_yearly[df_salary_yearly[i + ' bool'] == True]['mean salary']))\n",
    "    word_means.append(np.mean(df_salary_yearly[df_salary_yearly[i + ' bool'] == True]['mean salary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print len(word_medians)\n",
    "print len(word_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130000.0, 125000.0, 150000.0, 75000.0, 60000.0, 135000.0, 135000.0, 170000.0, 150000.0, 170000.0, 148750.0, 172500.0, 135000.0, 78790.5, 80000.0, 96250.0, 137500.0, 73570.5, 93664.0, 83421.5, 85000.0, 145000.0, 132500.0, 75557.5, 125000.0, 147500.0, 115000.0, 122500.0, 120000.0, 55000.0, 120000.0, 162500.0, 130000.0, 54095.5, 91740.5, 99624.75, 125000.0, 50000.0, 120000.0, 130000.0, 85000.0, 88746.5, 162500.0, 72500.0, 98750.0, 55000.0, 38400.0, 93750.0, 120000.0, 127500.0]\n"
     ]
    }
   ],
   "source": [
    "print word_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_features_df['word medians'] = word_medians\n",
    "important_features_df['word means'] = word_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>word medians</th>\n",
       "      <th>word means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.089219</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>135788.343254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.068727</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>127673.600418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.065603</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>149711.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>0.063372</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>85742.476510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.059086</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>72053.408730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           importance  word medians     word means\n",
       "data         0.089219      130000.0  135788.343254\n",
       "scientist    0.068727      125000.0  127673.600418\n",
       "learning     0.065603      150000.0  149711.538462\n",
       "analyst      0.063372       75000.0   85742.476510\n",
       "research     0.059086       60000.0   72053.408730"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108607.5"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(df_salary_yearly['mean salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "median_effect = []\n",
    "for i in important_features_df['word medians']:\n",
    "    if i > 108607.5:\n",
    "        median_effect.append('+')\n",
    "    else:\n",
    "        median_effect.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(median_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features_df['word effect'] = median_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>word medians</th>\n",
       "      <th>word means</th>\n",
       "      <th>word effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.089219</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>135788.343254</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.068727</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>127673.600418</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.065603</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>149711.538462</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>0.063372</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>85742.476510</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.059086</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>72053.408730</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.046728</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>137642.873684</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senior</th>\n",
       "      <td>0.040906</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>134602.788043</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantitative</th>\n",
       "      <td>0.036615</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>164760.000000</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <td>0.031660</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>148725.490196</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.017574</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>178333.333333</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0.015626</td>\n",
       "      <td>148750.0</td>\n",
       "      <td>152751.928571</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director</th>\n",
       "      <td>0.012119</td>\n",
       "      <td>172500.0</td>\n",
       "      <td>172032.562500</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>0.011145</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>120893.750000</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>0.010899</td>\n",
       "      <td>78790.5</td>\n",
       "      <td>77260.423077</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical</th>\n",
       "      <td>0.009820</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>87546.239130</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark</th>\n",
       "      <td>0.007950</td>\n",
       "      <td>96250.0</td>\n",
       "      <td>96250.000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit</th>\n",
       "      <td>0.007711</td>\n",
       "      <td>137500.0</td>\n",
       "      <td>137500.000000</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>0.007597</td>\n",
       "      <td>73570.5</td>\n",
       "      <td>65485.708333</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>0.007450</td>\n",
       "      <td>93664.0</td>\n",
       "      <td>96336.083333</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinator</th>\n",
       "      <td>0.007290</td>\n",
       "      <td>83421.5</td>\n",
       "      <td>76720.357143</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               importance  word medians     word means word effect\n",
       "data             0.089219      130000.0  135788.343254           +\n",
       "scientist        0.068727      125000.0  127673.600418           +\n",
       "learning         0.065603      150000.0  149711.538462           +\n",
       "analyst          0.063372       75000.0   85742.476510           -\n",
       "research         0.059086       60000.0   72053.408730           -\n",
       "engineer         0.046728      135000.0  137642.873684           +\n",
       "senior           0.040906      135000.0  134602.788043           +\n",
       "quantitative     0.036615      170000.0  164760.000000           +\n",
       "machine          0.031660      150000.0  148725.490196           +\n",
       "big              0.017574      170000.0  178333.333333           +\n",
       "science          0.015626      148750.0  152751.928571           +\n",
       "director         0.012119      172500.0  172032.562500           +\n",
       "risk             0.011145      135000.0  120893.750000           +\n",
       "environmental    0.010899       78790.5   77260.423077           -\n",
       "statistical      0.009820       80000.0   87546.239130           -\n",
       "spark            0.007950       96250.0   96250.000000           -\n",
       "credit           0.007711      137500.0  137500.000000           +\n",
       "laboratory       0.007597       73570.5   65485.708333           -\n",
       "clinical         0.007450       93664.0   96336.083333           -\n",
       "coordinator      0.007290       83421.5   76720.357143           -"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features_df.to_csv('../csvs/features_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>word medians</th>\n",
       "      <th>word means</th>\n",
       "      <th>word effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.089219</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>135788.343254</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientist</th>\n",
       "      <td>0.068727</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>127673.600418</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.065603</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>149711.538462</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyst</th>\n",
       "      <td>0.063372</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>85742.476510</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>0.059086</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>72053.408730</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           importance  word medians     word means word effect\n",
       "data         0.089219      130000.0  135788.343254           +\n",
       "scientist    0.068727      125000.0  127673.600418           +\n",
       "learning     0.065603      150000.0  149711.538462           +\n",
       "analyst      0.063372       75000.0   85742.476510           -\n",
       "research     0.059086       60000.0   72053.408730           -"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances.head(20).to_csv('feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2)\n",
    "dt.fit(cvec_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"930pt\" height=\"358pt\"\n",
       " viewBox=\"0.00 0.00 930.04 358.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 354)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-354 926.0371,-354 926.0371,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"521.8066,-350 402.7306,-350 402.7306,-286 521.8066,-286 521.8066,-350\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.2686\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[107] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"462.2686\" y=\"-320.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"462.2686\" y=\"-306.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 649</text>\n",
       "<text text-anchor=\"middle\" x=\"462.2686\" y=\"-292.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [325, 324]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"410.8066,-250 291.7306,-250 291.7306,-186 410.8066,-186 410.8066,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[256] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.4488</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 400</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [264, 136]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M426.5365,-285.8089C416.3368,-276.62 405.1125,-266.508 394.5155,-256.9612\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"396.706,-254.2238 386.9337,-250.1308 392.0207,-259.4245 396.706,-254.2238\"/>\n",
       "<text text-anchor=\"middle\" x=\"388.2352\" y=\"-270.8969\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"640.3066,-250 528.2305,-250 528.2305,-186 640.3066,-186 640.3066,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[29] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.3699</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 249</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [61, 188]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M501.5417,-285.8089C512.861,-276.5308 525.3284,-266.3116 537.0749,-256.6833\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"539.5538,-259.177 545.069,-250.1308 535.1163,-253.7632 539.5538,-259.177\"/>\n",
       "<text text-anchor=\"middle\" x=\"542.6036\" y=\"-270.8089\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"239.8066,-150 120.7306,-150 120.7306,-86 239.8066,-86 239.8066,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[348] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.3999</text>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 362</text>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [262, 100]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M296.2218,-185.8089C279.5935,-176.0848 261.1974,-165.3268 244.0496,-155.2989\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"245.6113,-152.1576 235.2122,-150.1308 242.0776,-158.2002 245.6113,-152.1576\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"400.3067,-150 302.2304,-150 302.2304,-86 400.3067,-86 400.3067,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[400] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0997</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 38</text>\n",
       "<text text-anchor=\"middle\" x=\"351.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 36]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M351.2686,-185.8089C351.2686,-177.6906 351.2686,-168.8517 351.2686,-160.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"354.7687,-160.1307 351.2686,-150.1308 347.7687,-160.1308 354.7687,-160.1307\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"112.3066,-50 .2305,-50 .2305,0 112.3066,0 112.3066,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.3713</text>\n",
       "<text text-anchor=\"middle\" x=\"56.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 341</text>\n",
       "<text text-anchor=\"middle\" x=\"56.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [257, 84]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.5948,-85.9947C124.722,-76.3401 110.6141,-65.7592 97.8157,-56.1604\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.7206,-53.2141 89.6206,-50.014 95.5206,-58.8141 99.7206,-53.2141\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"229.3067,-50 131.2304,-50 131.2304,0 229.3067,0 229.3067,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.3628</text>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 21</text>\n",
       "<text text-anchor=\"middle\" x=\"180.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 16]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M180.2686,-85.9947C180.2686,-77.6273 180.2686,-68.5643 180.2686,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"183.7687,-60.014 180.2686,-50.014 176.7687,-60.0141 183.7687,-60.014\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"346.3067,-50 248.2304,-50 248.2304,0 346.3067,0 346.3067,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"297.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0526</text>\n",
       "<text text-anchor=\"middle\" x=\"297.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 37</text>\n",
       "<text text-anchor=\"middle\" x=\"297.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 36]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.6848,-85.9947C327.6128,-77.2595 322.1005,-67.7662 316.9687,-58.928\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"319.841,-56.9044 311.7928,-50.014 313.7875,-60.4194 319.841,-56.9044\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"455.8068,-50 364.7303,-50 364.7303,0 455.8068,0 455.8068,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"410.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"410.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"410.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M371.573,-85.9947C377.173,-77.1676 383.2642,-67.5662 388.9213,-58.6491\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.9978,-60.3331 394.3994,-50.014 386.087,-56.5832 391.9978,-60.3331\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"640.3066,-150 528.2305,-150 528.2305,-86 640.3066,-86 640.3066,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[357] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.2876</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 224</text>\n",
       "<text text-anchor=\"middle\" x=\"584.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [39, 185]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M584.2686,-185.8089C584.2686,-177.6906 584.2686,-168.8517 584.2686,-160.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"587.7687,-160.1307 584.2686,-150.1308 580.7687,-160.1308 587.7687,-160.1307\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"812.3067,-150 714.2304,-150 714.2304,-86 812.3067,-86 812.3067,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[196] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.2112</text>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 25</text>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [22, 3]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M640.4142,-186.6337C661.0481,-175.1064 684.4422,-162.0371 705.2386,-150.419\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"706.9536,-153.4701 713.9767,-145.5374 703.5396,-147.359 706.9536,-153.4701\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"586.3066,-50 474.2305,-50 474.2305,0 586.3066,0 586.3066,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"530.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.2727</text>\n",
       "<text text-anchor=\"middle\" x=\"530.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 221</text>\n",
       "<text text-anchor=\"middle\" x=\"530.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [36, 185]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M565.6848,-85.9947C560.6128,-77.2595 555.1005,-67.7662 549.9687,-58.928\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"552.841,-56.9044 544.7928,-50.014 546.7875,-60.4194 552.841,-56.9044\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"695.8068,-50 604.7303,-50 604.7303,0 695.8068,0 695.8068,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"650.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"650.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"650.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 0]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M606.982,-85.9947C613.3116,-77.0756 620.2023,-67.3661 626.5862,-58.3706\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"629.5834,-60.1947 632.5167,-50.014 623.8749,-56.1435 629.5834,-60.1947\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"812.3067,-50 714.2304,-50 714.2304,0 812.3067,0 812.3067,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0832</text>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 23</text>\n",
       "<text text-anchor=\"middle\" x=\"763.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [22, 1]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.2686,-85.9947C763.2686,-77.6273 763.2686,-68.5643 763.2686,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.7687,-60.014 763.2686,-50.014 759.7687,-60.0141 766.7687,-60.014\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"921.8068,-50 830.7303,-50 830.7303,0 921.8068,0 921.8068,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"876.2686\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"876.2686\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"876.2686\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M802.1567,-85.9947C813.7759,-76.432 826.4992,-65.9606 838.0735,-56.4349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"840.378,-59.0712 845.8752,-50.014 835.9297,-53.6663 840.378,-59.0712\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x113e38490>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "export_graphviz(dt, out_file=\"mytree.dot\")\n",
    "with open(\"mytree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 459)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(clf.feature_importances_ == np.mean([tree.feature_importances_ for tree in clf.estimators_], axis=0))\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "# calculate the standard deviation of feature importances by looping over the trees in the random forest\n",
    "# \n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_cvec_train.columns\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_cvec_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_cvec_train.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X_cvec_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Search Term</th>\n",
       "      <th>lower_sal_val</th>\n",
       "      <th>upper_sal_val</th>\n",
       "      <th>mean salary</th>\n",
       "      <th>median_bool</th>\n",
       "      <th>60_perc_bool</th>\n",
       "      <th>70_perc_bool</th>\n",
       "      <th>75_perc_bool</th>\n",
       "      <th>90_perc_bool</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Platinum Solutions</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>80000 - 120000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>80000</td>\n",
       "      <td>120000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bioinformatics Software Developer</td>\n",
       "      <td>Genialis</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>50000 - 80000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>50000</td>\n",
       "      <td>80000</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quality Assurance Analyst (Research)</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>43794</td>\n",
       "      <td>Houston</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794</td>\n",
       "      <td>43794.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flow Cytometry Specialist I</td>\n",
       "      <td>Baylor College of Medicine</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>46831</td>\n",
       "      <td>Houston</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831</td>\n",
       "      <td>46831.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Statistical Analyst -Bioinformatics &amp;...</td>\n",
       "      <td>MD Anderson Cancer Center</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>66400 - 99600</td>\n",
       "      <td>Houston</td>\n",
       "      <td>66400</td>\n",
       "      <td>99600</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>'TX'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                                     Data Scientist   \n",
       "1                  Bioinformatics Software Developer   \n",
       "2               Quality Assurance Analyst (Research)   \n",
       "3                        Flow Cytometry Specialist I   \n",
       "4  Research Statistical Analyst -Bioinformatics &...   \n",
       "\n",
       "                              Company     Location          Salary  \\\n",
       "0                  Platinum Solutions  Houston, TX  80000 - 120000   \n",
       "1                            Genialis  Houston, TX   50000 - 80000   \n",
       "2          Baylor College of Medicine  Houston, TX           43794   \n",
       "3          Baylor College of Medicine  Houston, TX           46831   \n",
       "4           MD Anderson Cancer Center  Houston, TX   66400 - 99600   \n",
       "\n",
       "  Search Term  lower_sal_val  upper_sal_val  mean salary median_bool  \\\n",
       "0     Houston          80000         120000     100000.0       False   \n",
       "1     Houston          50000          80000      65000.0       False   \n",
       "2     Houston          43794          43794      43794.0       False   \n",
       "3     Houston          46831          46831      46831.0       False   \n",
       "4     Houston          66400          99600      83000.0       False   \n",
       "\n",
       "  60_perc_bool 70_perc_bool 75_perc_bool 90_perc_bool state  \n",
       "0        False        False        False        False  'TX'  \n",
       "1        False        False        False        False  'TX'  \n",
       "2        False        False        False        False  'TX'  \n",
       "3        False        False        False        False  'TX'  \n",
       "4        False        False        False        False  'TX'  "
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9c9274ef-c9f5-4d56-b286-ecc8709eff9f"
   },
   "source": [
    "#### Rebuild this model with the new variables\n",
    "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n",
    "- Remember to scale the feature variables as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "b76f65cd-cd3a-4e91-af55-12880be7b057"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "56cc8854-d722-411d-a6c7-e86310710f67"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "fead9b5b-7316-405d-87fd-e144dff0cbeb"
   },
   "source": [
    "#### Continue to incorporate other text features from the title or summary that you believe will predict the salary and examine their coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "12d5edeb-a272-43a0-9977-d951f12fedfb"
   },
   "source": [
    "#### Take ~100 scraped entries with salaries. Convert them to use with your model and predict the salary - which entries have the highest predicted salaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3be94357-e551-4094-b784-2df039216d33"
   },
   "source": [
    "### BONUS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "#### Bonus: Use Count Vectorizer from scikit-learn to create features from the text summaries. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "4239e458-28bd-4675-8db3-c1d9c02b9854"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "fec80936-37bc-4922-89bd-b5d615566c9c"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
